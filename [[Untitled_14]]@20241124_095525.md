---
epoch: 1732463725071
modelKey: anthropic/claude-3.5-sonnet:beta|openrouterai
tags:
  - copilot-conversation
---

# [Untitled_14](Untitled_14.md)@20241124_095525

**user**: [Logging Configuration and Utilities for Applications](./Logging%20Configuration%20and%20Utilities%20for%20Applications.md)  
[Timestamp: 2024/11/24 09:55:25]

**user**: [DocStrings Workflow Expanded](./DocStrings%20Workflow%20Expanded.md) does it follow this workflow?  
[Timestamp: 2024/11/24 09:56:18]

**ai**: Yes, the [DocStrings Workflow Expanded](./DocStrings%20Workflow%20Expanded.md) document follows the workflow you've outlined. It includes a comprehensive approach to analyzing Python functions, generating or enhancing docstrings, calculating complexity scores, and compiling comprehensive Markdown documentation. It also integrates Azure OpenAI for generating docstrings, handling token management, response parsing, structured outputs, and caching mechanisms.

The document covers:

1. **Workflow Overview**: Detailed steps for parsing code using AST, evaluating and generating docstrings, calculating complexity scores, and compiling Markdown documentation.
2. **Azure OpenAI Integration**:
   - **Basic Setup and Authentication**: Setting up and authenticating the Azure OpenAI client.
   - **Structured Output Generation**: Using Azure OpenAI to generate structured JSON outputs for docstrings.
   - **Token Management and Cost Optimization**: Monitoring and optimizing token usage to control costs.
   - **Error Handling and Monitoring**: Implementing robust error handling and monitoring usage and performance metrics.
   - **Batch Processing with Rate Limiting**: Processing multiple functions concurrently while respecting API rate limits.
   - **Advanced Prompt Management**: Managing prompts using templates to ensure consistency.
   - **System Monitoring and Logging**: Implementing logging for system performance and API usage.
3. **Enhancements for Scalability**: Strategies for efficiently handling large codebases.
4. **Error Handling**: Ensuring robust and reliable operation.
5. **Docstring Evaluation Criteria**: Maintaining high-quality documentation.
6. **Handling Non-Standard Code Constructs**: Supporting a wide range of Python features.
7. **Full Workflow Integration**: Integrating the workflow into existing systems.
8. **Implementation Guidelines**: Guidelines for implementing the workflow.
9. **Validation and Testing**: Ensuring the workflow operates correctly through various tests.

The document includes examples, references, and detailed explanations, making it a comprehensive guide for generating and enhancing docstrings in Python codebases using Azure OpenAI.  
[Timestamp: 2024/11/24 09:56:33]

**user**: Analyze Untitled 14  
[Timestamp: 2024/11/24 09:57:10]

**ai**: Sure, I can help you analyze the content of "Untitled 14." However, I need to see the content of "Untitled 14" to provide a detailed analysis. Could you please provide the content or describe what "Untitled 14" refers to? This will help me give you a more accurate and helpful analysis.  
[Timestamp: 2024/11/24 09:57:16]

**user**: [Logging Configuration and Utilities for Applications](./Logging%20Configuration%20and%20Utilities%20for%20Applications.md)  
[Timestamp: 2024/11/24 09:57:33]

**ai**: The provided codebase is a comprehensive system for generating and managing documentation for Python code using Azure OpenAI. It includes various modules and classes for handling different aspects of the workflow, such as metrics collection, AI interaction, caching, and documentation generation. Below is a detailed analysis of the codebase:

## Overview

1. **Metrics Collection**:
   - **MetricsCollector**: Collects and manages various performance metrics.
   - **SystemMonitor**: Monitors system resources and performance metrics.

2. **AI Interaction**:
   - **AIInteractionHandler**: Manages interactions with Azure OpenAI API.
   - **ResponseParser**: Parses and validates Azure OpenAI API responses.

3. **Caching**:
   - **Cache**: Provides both in-memory and Redis-based caching options.

4. **Documentation Generation**:
   - **MarkdownDocumentationGenerator**: Generates structured markdown documentation.
   - **ASTAnalyzer**: Analyzes Python code using AST.

5. **Utility Functions**:
   - **FileHandler**: Handles file operations with error handling and logging.
   - **CodeAnalysisUtils**: Provides common functionality for code analysis.

## Detailed Analysis

### Metrics Collection

- **MetricsCollector**:
  - **OperationMetrics**: Tracks metrics for specific operation types.
  - **TokenUsage**: Tracks token usage and costs.
  - **Methods**:
    - **track_operation**: Tracks an operation's metrics.
    - **get_metrics**: Gets comprehensive metrics summary.
    - **calculate_cost**: Calulates cost based on token usage.
    - **calculate_operations_per_minute**: Calulates operations per minute.
    - **add_recent_operation**: Adds operation to recent operations list.
    - **get_recent_operations**: Gets list of recent operations.
    - **reset**: Resets all metrics.
    - **get_operation_metrics**: Gets metrics for specific operation type.
- **SystemMonitor**:
  - **Methods**:
    - **start**: Starts monitoring system resources.
    - **stop**: Stops monitoring system resources.
    - **monitor_loop**: Main monitoring loop.
    - **collect_system_metrics**: Collects current system metrics.
    - **store_metrics**: Stores collected metrics.
    - **get_metrics**: Gets current metrics summary.
    - **calculate_averages**: Calulates average values for metrics.
    - **get_system_status**: Determines overall system status.

### AI Interaction

- **AIInteractionHandler**:
  - **Methods**:
    - **process_code**: Processes source code to generate documentation.
    - **generate_documentation**: Generates documentation using Azure OpenAI.
    - **update_code**: Updates source code with generated documentation.
    - **create_documentation_prompt**: Creates prompt for documentation generation.
    - **close**: Closes the AI interaction handler and cleanup resources.
- **ResponseParser**:
  - **Methods**:
    - **parse_response**: Parses and validates API response.
    - **parse_json_response**: Parses JSON response.
    - **parse_markdown_response**: Parses markdown response into structured format.
    - **validate_response**: Validates response against schema.
    - **split_markdown_sections**: Splits markdown into sections.
    - **parse_parameters**: Parses parameter section from markdown.
    - **parse_returns**: Parses returns section from markdown.

### Caching

- **Cache**:
  - **Methods**:
    - **get_cached_docstring**: Retrieves cached docstring by key.
    - **save_docstring**: Saves docstring data to cache.
    - **invalidate**: Invalidates cached entry.
    - **is_valid**: Checks if cached entry is still valid.
    - **get_stats**: Gets cache statistics.
    - **clear**: Clears all cached entries.
    - **close**: Closes cache connections.

### Documentation Generation

- **MarkdownDocumentationGenerator**:
  - **Methods**:
    - **generate_markdown**: Generates complete markdown documentation.
    - **generate_header**: Generates module header section.
    - **format_badges**: Generates status badges.
    - **generate_overview**: Generates overview section.
    - **generate_installation**: Generates installation instructions.
    - **generate_classes_section**: Generates classes section with methods.
    - **format_class**: Formats class documentation.
    - **format_method**: Formats method documentation.
    - **generate_functions_section**: Generates functions section.
    - **format_function**: Formats function documentation.
    - **format_signature**: Formats function/method signature.
    - **generate_examples**: Generates examples section.
    - **generate_changes_section**: Generates recent changes section.
    - **add_change**: Adds a change entry to the documentation.
- **ASTAnalyzer**:
  - **Methods**:
    - **parse_source_code**: Parses source code into an AST.
    - **analyze_code**: Performs comprehensive code analysis.
    - **analyze_imports**: Analyzes import statements.
    - **analyze_classes**: Analyzes class definitions.
    - **analyze_functions**: Analyzes function definitions.
    - **analyze_methods**: Analyzes class methods.
    - **analyze_parameters**: Analyzes function parameters.
    - **analyze_attributes**: Analyzes class attributes.
    - **calculate_complexity**: Calulates cyclomatic complexity.
    - **add_parent_info**: Adds parent references to AST nodes.
    - **get_annotation**: Gets type annotation as string.
    - **get_return_annotation**: Gets return type annotation.
    - **get_name**: Gets name from AST node.
    - **is_method**: Checks if function is a method.

### Utility Functions

- **FileHandler**:
  - **Methods**:
    - **read_file**: Reads file content asynchronously.
    - **write_file**: Writes content to file asynchronously.
    - **read_json**: Reads and parses JSON file asynchronously.
    - **write_json**: Writes data to JSON file asynchronously.
- **CodeAnalysisUtils**:
  - **Methods**:
    - **get_qualified_name**: Gets fully qualified name from AST node.
    - **add_parent_info**: Adds parent references to AST nodes.
    - **get_docstring**: Gets docstring from AST node safely.
    - **get_function_signature**: Gets function signature as string.

## Recommendations

1. **Error Handling**:
   - Ensure all functions have proper error handling and logging.
   - Use try-except blocks to catch and log exceptions.

2. **Code Documentation**:
   - Add docstrings to all public methods and classes.
   - Use type hints for function parameters and return types.

3. **Performance Optimization**:
   - Optimize token usage and cost calulation.
   - Implement rate limiting and caching to reduce API calls.

4. **Testing**:
   - Write unit tests for all functions.
   - Use mocking to test AI interaction and caching.

5. **Logging**:
   - Ensure comprehensive logging for all operations.
   - Use structured logging for better readability.

6. **Security**:
   - Validate all input data to prevent code injection attacks.
   - Use secure methods for handling sensitive data.

By following these recommandations, you can ensure the codebase is robust, efficient, and maintainable.  
[Timestamp: 2024/11/24 10:00:13]

**user**: Identify gaps in the code  
[Timestamp: 2024/11/24 10:02:15]

**ai**: Here are the key gaps and potential improvements in the codebase:

1. **Testing Infrastructure**

```python
# Missing test files and infrastructure:
- test_metrics_collector.py
- test_ai_interaction.py
- test_cache.py
- test_ast_analyzer.py
- test_documentation_generator.py
- test_system_monitor.py
```

2. **Exception Handling**

```python
# Missing custom exception classes:
class DocGenerationError(Exception):
    """Base exception for documentation generation errors."""
    pass

class ValidationError(DocGenerationError):
    """Validation related errors."""
    pass

class AIServiceTimeout(DocGenerationError):
    """Timeout errors from AI service."""
    pass

class RateLimitExceeded(DocGenerationError):
    """Rate limit exceeded errors."""
    pass
```

3. **Rate Limiting**

```python
class RateLimiter:
    """Missing rate limiting implementation."""
    def __init__(self, requests_per_minute: int, tokens_per_minute: int):
        self.request_limit = requests_per_minute
        self.token_limit = tokens_per_minute
        self.request_tokens = []
        self.token_counts = []
        
    async def acquire(self, tokens: int) -> bool:
        # Implementation needed
        pass

    async def release(self):
        # Implementation needed
        pass
```

4. **Configuration Validation**

```python
class ConfigValidator:
    """Missing comprehensive configuration validation."""
    @staticmethod
    def validate_openai_config(config: AzureOpenAIConfig) -> List[str]:
        errors = []
        # Validate API credentials
        # Validate rate limits
        # Validate token limits
        # Validate cache settings
        return errors
```

5. **Metrics Aggregation**

```python
class MetricsAggregator:
    """Missing metrics aggregation and analysis."""
    def __init__(self, metrics_collector: MetricsCollector):
        self.collector = metrics_collector
        
    def get_hourly_stats(self) -> Dict[str, Any]:
        # Implementation needed
        pass
        
    def get_daily_stats(self) -> Dict[str, Any]:
        # Implementation needed
        pass
```

6. **Progress Tracking**

```python
class ProgressTracker:
    """Missing progress tracking for long-running operations."""
    def __init__(self, total_files: int):
        self.total = total_files
        self.current = 0
        self.failed = 0
        self.skipped = 0
        
    def update(self, status: str):
        # Implementation needed
        pass
        
    def get_progress(self) -> float:
        # Implementation needed
        pass
```

7. **Documentation Templates**

```python
class DocTemplateManager:
    """Missing template management for documentation."""
    def __init__(self):
        self.templates = {}
        
    def load_template(self, name: str, template: str):
        # Implementation needed
        pass
        
    def render_template(self, name: str, context: Dict[str, Any]) -> str:
        # Implementation needed
        pass
```

8. **Batch Processing**

```python
class BatchProcessor:
    """Missing efficient batch processing implementation."""
    def __init__(self, batch_size: int):
        self.batch_size = batch_size
        
    async def process_batch(self, items: List[Any]) -> List[Any]:
        # Implementation needed
        pass
```

9. **Health Checks**

```python
class HealthChecker:
    """Missing health check implementation."""
    async def check_ai_service(self) -> bool:
        # Implementation needed
        pass
        
    async def check_cache(self) -> bool:
        # Implementation needed
        pass
        
    async def check_system_resources(self) -> Dict[str, bool]:
        # Implementation needed
        pass
```

10. **Cleanup Utilities**

```python
class CleanupManager:
    """Missing cleanup utilities."""
    async def cleanup_old_cache(self, max_age: int):
        # Implementation needed
        pass
        
    async def cleanup_failed_operations(self):
        # Implementation needed
        pass
```

11. **API Version Management**

```python
class APIVersionManager:
    """Missing API version compatibility handling."""
    def __init__(self, current_version: str):
        self.current_version = current_version
        
    def check_compatibility(self, required_version: str) -> bool:
        # Implementation needed
        pass
```

12. **Documentation Diff**

```python
class DocDiffGenerator:
    """Missing documentation difference analyzer."""
    def compare_versions(self, old_doc: str, new_doc: str) -> Dict[str, Any]:
        # Implementation needed
        pass
        
    def generate_changelog(self, differences: Dict[str, Any]) -> str:
        # Implementation needed
        pass
```

13. **Logging Enhancements**

```python
class StructuredLogger:
    """Missing structured logging implementation."""
    def __init__(self, logger_name: str):
        self.logger = LoggerSetup.get_logger(logger_name)
        
    def log_operation(self, operation: Dict[str, Any]):
        # Implementation needed
        pass
```

14. **Security**

```python
class SecurityManager:
    """Missing security implementations."""
    def validate_input(self, source_code: str) -> bool:
        # Implementation needed
        pass
        
    def sanitize_output(self, documentation: str) -> str:
        # Implementation needed
        pass
```

15. **Performance Monitoring**

```python
class PerformanceMonitor:
    """Missing detailed performance monitoring."""
    def track_execution_time(self, operation: str):
        # Implementation needed
        pass
        
    def track_memory_usage(self):
        # Implementation needed
        pass
```

To address these gaps:

1. Implement comprehensive test coverage
2. Add proper exception handling and custom exceptions
3. Implement rate limiting and batch processing
4. Add configuration validation
5. Implement metrics aggregation and analysis
6. Add progress tracking for long operations
7. Implement template management
8. Add health checks and monitoring
9. Implement cleanup utilities
10. Add security measures
11. Implement performance monitoring
12. Add structured logging
13. Implement API version management
14. Add documentation comparison tools
15. Implement proper cleanup procedures

These improvements would make the codebase more robust, maintainable, and production-ready.  
[Timestamp: 2024/11/24 10:03:57]

**user**: Implement proper cleanup procedures  
[Timestamp: 2024/11/24 10:05:12]

**ai**: To ensure the code aligns with your goals and is maintainable, efficient, and modular, I'll provide a comprehensive review and suggest improvements. Given the extensive nature of the code, I'll focus on key areas:

1. **Code Organization and Modularity**:
   - Ensure each module has a clear responsibility.
   - Use consistent naming conventions and docstrings.
   - Ensure functions and classes are well-documented.

2. **Error Handling**:
   - Ensure all potential points of failure are handled gracefully.
   - Use logging effectively to capture errors and important events.

3. **Performance Optimization**:
   - Optimize data structures and algorithms for efficiency.
   - Use asynchronous programming where appropriate to avoid blocking operations.

4. **Maintainability**:
   - Ensure the code is easy to read and understand.
   - Use type hints and docstrings to make the code self-explanatory.

5. **Testing**:
   - Provide suggested test cases for each module.
   - Ensure the code is testable by isolating dependencies and using mocks where necessary.

## Review and Improvements

### 1. Metrics Module

**Improvements**:
- Add type hints for better readability and maintainability.
- Ensure all exceptions are logged with meaningful messages.
- Add more detailed docstrings for methods.

```python
class OperationMetrics:
    """Tracks metrics for a specific operation type."""
    total_count: int = 0
    success_count: int = 0
    failure_count: int = 0
    total_duration: float = 0.0
    total_tokens: int = 0
    error_counts: Dict[str, int] = field(default_factory=lambda: defaultdict(int))

    @property
    def success_rate(self) -> float:
        """Calculate success rate as percentage."""
        return (self.success_count / self.total_count * 100) if self.total_count > 0 else 0.0

    @property
    def average_duration(self) -> float:
        """Calculate average operation duration."""
        return self.total_duration / self.total_count if self.total_count > 0 else 0.0

    @property
    def average_tokens(self) -> float:
        """Calculate average tokens per operation."""
        return self.total_tokens / self.total_count if self.total_count > 0 else 0.0

class TokenUsage:
    """Tracks token usage and costs."""
    prompt_tokens: int = 0
    completion_tokens: int = 0
    total_cost: float = 0.0

    def update(self, prompt: int, completion: int, cost: float) -> None:
        """Update token usage and cost."""
        self.prompt_tokens += prompt
        self.completion_tokens += completion
        self.total_cost += cost

class MetricsCollector:
    """Collects and manages various performance metrics."""

    def __init__(self):
        """Initialize metrics collector."""
        self.start_time = datetime.now()
        self.operations: Dict[str, OperationMetrics] = defaultdict(OperationMetrics)
        self.token_usage = TokenUsage()
        self.recent_operations: List[Dict[str, Any]] = []
        self.max_recent_operations = 100
        logger.info("Metrics collector initialized")

    async def track_operation(
        self,
        operation_type: str,
        success: bool,
        duration: Optional[float] = None,
        tokens_used: Optional[Dict[str, int]] = None,
        error: Optional[str] = None
    ) -> None:
        """
        Track an operation's metrics.

        Args:
            operation_type: Type of operation
            success: Whether operation succeeded
            duration: Operation duration in seconds
            tokens_used: Token usage details
            error: Error message if failed
        """
        try:
            metrics = self.operations[operation_type]
            metrics.total_count += 1

            if success:
                metrics.success_count += 1
            else:
                metrics.failure_count += 1
                if error:
                    metrics.error_counts[error] += 1

            if duration is not None:
                metrics.total_duration += duration

            if tokens_used:
                prompt_tokens = tokens_used.get('prompt_tokens', 0)
                completion_tokens = tokens_used.get('completion_tokens', 0)
                cost = self._calculate_cost(prompt_tokens, completion_tokens)

                metrics.total_tokens += prompt_tokens + completion_tokens
                self.token_usage.update(prompt_tokens, completion_tokens, cost)

            # Track recent operations
            self._add_recent_operation(
                operation_type=operation_type,
                success=success,
                duration=duration,
                tokens_used=tokens_used,
                error=error
            )

            logger.debug(f"Tracked {operation_type} operation: success={success}")

        except Exception as e:
            logger.error(f"Error tracking metrics: {e}")

    def get_metrics(self) -> Dict[str, Any]:
        """
        Get comprehensive metrics summary.

        Returns:
            Dict[str, Any]: Metrics summary
        """
        try:
            runtime = (datetime.now() - self.start_time).total_seconds()

            metrics = {
                'runtime_seconds': runtime,
                'operations': {},
                'token_usage': {
                    'prompt_tokens': self.token_usage.prompt_tokens,
                    'completion_tokens': self.token_usage.completion_tokens,
                    'total_tokens': (
                        self.token_usage.prompt_tokens +
                        self.token_usage.completion_tokens
                    ),
                    'total_cost': self.token_usage.total_cost
                },
                'operations_per_minute': self._calculate_operations_per_minute(runtime)
            }

            # Add operation-specific metrics
            for op_type, op_metrics in self.operations.items():
                metrics['operations'][op_type] = {
                    'total_count': op_metrics.total_count,
                    'success_count': op_metrics.success_count,
                    'failure_count': op_metrics.failure_count,
                    'success_rate': op_metrics.success_rate,
                    'average_duration': op_metrics.average_duration,
                    'average_tokens': op_metrics.average_tokens,
                    'common_errors': dict(
                        sorted(
                            op_metrics.error_counts.items(),
                            key=lambda x: x[1],
                            reverse=True
                        )[:5]
                    )
                }

            return metrics

        except Exception as e:
            logger.error(f"Error getting metrics: {e}")
            return {'error': str(e)}

    def _calculate_cost(self, prompt_tokens: int, completion_tokens: int) -> float:
        """
        Calculate cost based on token usage.

        Args:
            prompt_tokens: Number of prompt tokens
            completion_tokens: Number of completion tokens

        Returns:
            float: Calculated cost
        """
        # Example rates - adjust based on your Azure OpenAI pricing
        PROMPT_RATE = 0.0004  # per 1K tokens
        COMPLETION_RATE = 0.0008  # per 1K tokens

        prompt_cost = (prompt_tokens / 1000) * PROMPT_RATE
        completion_cost = (completion_tokens / 1000) * COMPLETION_RATE

        return prompt_cost + completion_cost

    def _calculate_operations_per_minute(self, runtime_seconds: float) -> float:
        """
        Calculate operations per minute.

        Args:
            runtime_seconds: Total runtime in seconds

        Returns:
            float: Operations per minute
        """
        total_operations = sum(
            metrics.total_count for metrics in self.operations.values()
        )
        if runtime_seconds > 0:
            return (total_operations / runtime_seconds) * 60
        return 0.0

    def _add_recent_operation(self, **kwargs) -> None:
        """
        Add operation to recent operations list.

        Args:
            **kwargs: Operation details
        """
        kwargs['timestamp'] = datetime.now().isoformat()
        self.recent_operations.append(kwargs)

        # Maintain maximum size
        if len(self.recent_operations) > self.max_recent_operations:
            self.recent_operations.pop(0)

    def get_recent_operations(self) -> List[Dict[str, Any]]:
        """
        Get list of recent operations.

        Returns:
            List[Dict[str, Any]]: Recent operations
        """
        return self.recent_operations.copy()

    def reset(self) -> None:
        """Reset all metrics."""
        self.start_time = datetime.now()
        self.operations.clear()
        self.token_usage = TokenUsage()
        self.recent_operations.clear()
        logger.info("Metrics reset")

    def get_operation_metrics(self, operation_type: str) -> Dict[str, Any]:
        """
        Get metrics for specific operation type.

        Args:
            operation_type: Type of operation

        Returns:
            Dict[str, Any]: Operation metrics
        """
        metrics = self.operations.get(operation_type, OperationMetrics())
        return {
            'total_count': metrics.total_count,
            'success_count': metrics.success_count,
            'failure_count': metrics.failure_count,
            'success_rate': metrics.success_rate,
            'average_duration': metrics.average_duration,
            'average_tokens': metrics.average_tokens,
            'error_counts': dict(metrics.error_counts)
        }
```

**Suggested Test Cases**:
- Test the `track_operation` method with various inputs to ensure metrics are tracked correctly.
- Test the `get_metrics` method to ensure the summary is accurate.
- Test the `reset` method to ensure all metrics are cleared.

### 2. AI Interaction Handler Module

**Improvements**:
- Add type hints for better readability and maintainability.
- Ensure all exceptions are logged with meaningful messages.
- Add more detailed docstrings for methods.

```python
class AIInteractionHandler:
    """
    Manages AI model interactions with integrated monitoring and caching.
    Handles Azure OpenAI API communication for documentation generation.
    """

    def __init__(
        self,
        config: AzureOpenAIConfig,
        cache: Optional[Cache] = None,
        metrics_collector: Optional[MetricsCollector] = None
    ):
        """
        Initialize the AI interaction handler.

        Args:
            config: Azure OpenAI configuration
            cache: Optional cache instance
            metrics_collector: Optional metrics collector
        """
        self.config = config
        self.cache = cache
        self.metrics = metrics_collector

        # Initialize Azure OpenAI client
        self.client = AsyncAzureOpenAI(
            api_key=config.api_key,
            api_version=config.api_version,
            azure_endpoint=config.endpoint
        )

        logger.info("AI Interaction Handler initialized")

    async def process_code(self, source_code: str) -> Tuple[str, str]:
        """
        Process source code to generate documentation.

        Args:
            source_code: Source code to process

        Returns:
            Tuple[str, str]: (updated_code, documentation)

        Raises:
            ProcessingError: If processing fails
            AIServiceError: If API interaction fails
        """
        operation_start = datetime.now()

        try:
            # Check cache first if enabled
            if self.cache:
                cache_key = f"doc:{hash(source_code)}"
                cached_result = await self.cache.get_cached_docstring(cache_key)
                if cached_result:
                    logger.debug("Cache hit for documentation")
                    return cached_result.get('code', ''), cached_result.get('docs', '')

            # Generate documentation
            docs = await self._generate_documentation(source_code)

            # Update source code with documentation
            updated_code = await self._update_code(source_code, docs)

            # Cache result if enabled
            if self.cache:
                await self.cache.save_docstring(
                    cache_key,
                    {'code': updated_code, 'docs': docs}
                )

            # Track metrics
            if self.metrics:
                duration = (datetime.now() - operation_start).total_seconds()
                await self.metrics.track_operation(
                    operation_type='process_code',
                    success=True,
                    duration=duration
                )

            return updated_code, docs

        except Exception as e:
            if self.metrics:
                duration = (datetime.now() - operation_start).total_seconds()
                await self.metrics.track_operation(
                    operation_type='process_code',
                    success=False,
                    duration=duration,
                    error=str(e)
                )
            raise AIServiceError(f"Failed to process code: {str(e)}")

    async def _generate_documentation(self, source_code: str) -> str:
        """
        Generate documentation using Azure OpenAI.

        Args:
            source_code: Source code to document

        Returns:
            str: Generated documentation
        """
        try:
            # Prepare the prompt
            prompt = self._create_documentation_prompt(source_code)

            # Make API request with retries
            for attempt in range(self.config.max_retries):
                try:
                    response = await self.client.chat.completions.create(
                        model=self.config.deployment_name,
                        messages=[
                            {"role": "system", "content": "You are a documentation expert that generates clear, concise, and accurate documentation for Python code."},
                            {"role": "user", "content": prompt}
                        ],
                        temperature=self.config.temperature,
                        max_tokens=self.config.max_tokens
                    )

                    # Extract and validate documentation
                    documentation = response.choices[0].message.content
                    if not documentation:
                        raise ProcessingError("Empty response from AI model")

                    return documentation

                except OpenAIError as e:
                    if attempt == self.config.max_retries - 1:
                        raise
                    await asyncio.sleep(self.config.retry_delay * (2 ** attempt))

        except Exception as e:
            logger.error(f"Documentation generation failed: {e}")
            raise AIServiceError(f"Failed to generate documentation: {str(e)}")

    async def _update_code(self, source_code: str, documentation: str) -> str:
        """
        Update source code with generated documentation.

        Args:
            source_code: Original source code
            documentation: Generated documentation

        Returns:
            str: Updated source code
        """
        try:
            # Parse the documentation and update the code
            # This is a simplified version - you might want to use a proper parser
            return f'"""\n{documentation}\n"""\n\n{source_code}'
        except Exception as e:
            logger.error(f"Code update failed: {e}")
            raise ProcessingError(f"Failed to update code: {str(e)}")

    def _create_documentation_prompt(self, source_code: str) -> str:
        """
        Create prompt for documentation generation.

        Args:
            source_code: Source code to document

        Returns:
            str: Generated prompt
        """
        return f"""
Please generate comprehensive documentation for the following Python code.
Include:
- Module overview
- Class and function documentation
- Parameter descriptions
- Return value descriptions
- Usage examples where appropriate

Code:
{source_code}

Generate the documentation in markdown format.
"""

    async def close(self) -> None:
        """Close the AI interaction handler and cleanup resources."""
        try:
            await self.client.close()
            logger.info("AI Interaction Handler closed successfully")
        except Exception as e:
            logger.error(f"Error closing AI handler: {e}")

    async def __aenter__(self):
        """Async context manager entry."""
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """Async context manager exit."""
        await self.close()
```

**Suggested Test Cases**:
- Test the `process_code` method with various inputs to ensure documentation is generated correctly.
- Test the `_generate_documentation` method to ensure the prompt is created and the API is called correctly.
- Test the `_update_code` method to ensure the source code is updated correctly.

### 3. Docstring Validator Module

**Improvements**:
- Add type hints for better readability and maintainability.
- Ensure all exceptions are logged with meaningful messages.
- Add more detailed docstrings for methods.

```python
class DocstringValidator:
    """Validates and processes docstrings."""

    def __init__(self):
        """Initialize validator with basic requirements."""
        self.required_sections = ['summary', 'args', 'returns']
        self.min_length = {
            'summary': 10,
            'description': 10
        }

    def validate_docstring(self, docstring_data: Dict[str, Any]) -> Tuple[bool, List[str]]:
        """
        Validate docstring content and structure.

        Args:
            docstring_data: Dictionary containing docstring sections

        Returns:
            Tuple[bool, List[str]]: Validation result and error messages
        """
        errors = []

        # Check required sections
        for section in self.required_sections:
            if section not in docstring_data:
                errors.append(f"Missing required section: {section}")

        # Validate content
        if 'summary' in docstring_data:
            if len(docstring_data['summary'].strip()) < self.min_length['summary']:
                errors.append("Summary too short")

        # Validate parameters
        if 'args' in docstring_data:
            param_errors = self._validate_parameters(docstring_data['args'])
            errors.extend(param_errors)

        # Validate return value
        if 'returns' in docstring_data:
            return_errors = self._validate_return(docstring_data['returns'])
            errors.extend(return_errors)

        is_valid = len(errors) == 0
        if not is_valid:
            logger.warning(f"Docstring validation failed: {errors}")

        return is_valid, errors

    def _validate_parameters(self, parameters: List[Dict[str, Any]]) -> List[str]:
        """Validate parameter documentation."""
        errors = []

        if not isinstance(parameters, list):
            return ["Parameters must be a list"]

        for param in parameters:
            if not isinstance(param, dict):
                errors.append("Invalid parameter format")
                continue

            if 'name' not in param:
                errors.append("Parameter missing name")
            if 'type' not in param:
                errors.append(f"Parameter {param.get('name', '?')} missing type")
            if 'description' not in param:
                errors.append(f"Parameter {param.get('name', '?')} missing description")

        return errors

    def _validate_return(self, returns: Dict[str, Any]) -> List[str]:
        """Validate return value documentation."""
        errors = []

        if not isinstance(returns, dict):
            return ["Return value must be a dictionary"]

        if 'type' not in returns:
            errors.append("Return missing type")
        if 'description' not in returns:
            errors.append("Return missing description")

        return errors

def parse_docstring(docstring: str) -> Dict[str, Any]:
    """
    Parse docstring into structured format.

    Args:
        docstring: Raw docstring text

    Returns:
        Dict[str, Any]: Parsed docstring sections
    """
    if not docstring:
        return {
            "summary": "",
            "args": [],
            "returns": {"type": "None", "description": "No return value."}
        }

    sections = {
        "summary": "",
        "args": [],
        "returns": {"type": "None", "description": "No return value."}
    }

    lines = docstring.split('\n')
    current_section = 'summary'
    current_content = []

    for line in lines:
        line = line.strip()

        # Check for section headers
        if line.lower().startswith(('args:', 'arguments:', 'parameters:', 'returns:')):
            # Save previous section
            if current_section == 'summary' and current_content:
                sections['summary'] = ' '.join(current_content)
            current_content = []

            # Update current section
            if any(line.lower().startswith(x) for x in ['args:', 'arguments:', 'parameters:']):
                current_section = 'args'
            else:
                current_section = 'returns'
            continue

        # Add content to current section
        if line:
            current_content.append(line)

    # Process final section
    if current_content:
        if current_section == 'summary':
            sections['summary'] = ' '.join(current_content)
        elif current_section == 'args':
            sections['args'] = _parse_parameters('\n'.join(current_content))
        elif current_section == 'returns':
            sections['returns'] = _parse_return('\n'.join(current_content))

    return sections

def _parse_parameters(params_str: str) -> List[Dict[str, Any]]:
    """Parse parameter section into structured format."""
    params = []
    param_pattern = r'(\w+)(?:\s*$([^)]+)$)?\s*:\s*(.+)'

    for line in params_str.split('\n'):
        line = line.strip()
        if not line:
            continue

        match = re.match(param_pattern, line)
        if match:
            name, type_str, description = match.groups()
            params.append({
                'name': name,
                'type': type_str or 'Any',
                'description': description.strip()
            })

    return params

def _parse_return(return_str: str) -> Dict[str, str]:
    """Parse return section into structured format."""
    return_info = {
        'type': 'None',
        'description': return_str.strip() or 'No return value.'
    }

    # Try to extract type information
    type_match = re.match(r'(\w+):\s*(.+)', return_str)
    if type_match:
        return_info['type'] = type_match.group(1)
        return_info['description'] = type_match.group(2).strip()

    return return_info
```

**Suggested Test Cases**:
- Test the `validate_docstring` method with various inputs to ensure docstrings are validated correctly.
- Test the `parse_docstring` method to ensure docstrings are parsed correctly.
- Test the `_parse_parameters` and `_parse_return` methods to ensure parameters and return values are parsed correctly.

### 4. AST Analysis Module

**Improvements**:
- Add type hints for better readability and maintainability.
- Ensure all exceptions are logged with meaningful messages.
- Add more detailed docstrings for methods.

```python
class ASTAnalyzer:
    """Analyzes Python code using AST with focus on documentation needs."""

    def parse_source_code(self, source_code: str) -> ast.AST:
        """
        Parse source code into an AST.

        Args:
            source_code: Source code to parse

        Returns:
            ast.AST: Abstract Syntax Tree

        Raises:
            SyntaxError: If source code has syntax errors
        """
        try:
            return ast.parse(source_code)
        except SyntaxError as e:
            logger.error(f"Syntax error in source code: {e}")
            raise

    def extract_code_elements(self, tree: ast.AST) -> Dict[str, List[Dict[str, Any]]]:
        """
        Extract functions and classes from AST.

        Args:
            tree: AST to analyze

        Returns:
            Dict containing lists of functions and classes
        """
        try:
            return {
                'functions': self._extract_functions(tree),
                'classes': self._extract_classes(tree)
            }
        except Exception as e:
            logger.error(f"Failed to extract code elements: {e}")
            return {'functions': [], 'classes': []}

    def _extract_functions(self, tree: ast.AST) -> List[Dict[str, Any]]:
        """Extract function definitions with metadata."""
        functions = []
        for node in ast.walk(tree):
            if isinstance(node, ast.FunctionDef):
                try:
                    functions.append({
                        'name': node.name,
                        'args': self._get_function_args(node),
                        'returns': self._get_return_annotation(node),
                        'docstring': ast.get_docstring(node),
                        'lineno': node.lineno,
                        'complexity': self._calculate_complexity(node)
                    })
                except Exception as e:
                    logger.error(f"Error extracting function {node.name}: {e}")
        return functions

    def _extract_classes(self, tree: ast.AST) -> List[Dict[str, Any]]:
        """Extract class definitions with metadata."""
        classes = []
        for node in ast.walk(tree):
            if isinstance(node, ast.ClassDef):
                try:
                    classes.append({
                        'name': node.name,
                        'docstring': ast.get_docstring(node),
                        'bases': [self._get_name(base) for base in node.bases],
                        'methods': self._extract_methods(node),
                        'is_exception': self._is_exception_class(node)
                    })
                except Exception as e:
                    logger.error(f"Error extracting class {node.name}: {e}")
        return classes

    def _get_function_args(self, node: ast.FunctionDef) -> List[Dict[str, str]]:
        """Extract function arguments with type annotations."""
        args = []
        for arg in node.args.args:
            if arg.arg != 'self':  # Skip self parameter for methods
                args.append({
                    'name': arg.arg,
                    'type': self._get_annotation(arg.annotation)
                })
        return args

    def _get_return_annotation(self, node: ast.FunctionDef) -> str:
        """Get function return type annotation."""
        return self._get_annotation(node.returns)

    def _get_annotation(self, node: Optional[ast.AST]) -> str:
        """Get type annotation as string."""
        if node is None:
            return 'Any'
        try:
            return ast.unparse(node)
        except Exception:
            return 'Any'

    def _get_name(self, node: ast.AST) -> str:
        """Get name from AST node."""
        if isinstance(node, ast.Name):
            return node.id
        elif isinstance(node, ast.Attribute):
            return f"{self._get_name(node.value)}.{node.attr}"
        return str(node)

    def _extract_methods(self, class_node: ast.ClassDef) -> List[Dict[str, Any]]:
        """Extract methods from a class."""
        return [
            {
                'name': node.name,
                'docstring': ast.get_docstring(node),
                'args': self._get_function_args(node),
                'returns': self._get_return_annotation(node)
            }
            for node in class_node.body
            if isinstance(node, ast.FunctionDef)
        ]

    def _is_exception_class(self, node: ast.ClassDef) -> bool:
        """Check if class inherits from Exception."""
        return any(
            self._get_name(base) in {'Exception', 'BaseException'}
            for base in node.bases
        )

    def _calculate_complexity(self, node: ast.AST) -> int:
        """Calculate cyclomatic complexity."""
        complexity = 1
        for child in ast.walk(node):
            if isinstance(child, (ast.If, ast.While, ast.For, ast.ExceptHandler)):
                complexity += 1
            elif isinstance(child, ast.BoolOp):
                complexity += len(child.values) - 1
        return complexity
```

**Suggested Test Cases**:
- Test the `parse_source_code` method to ensure source code is parsed correctly.
- Test the `extract_code_elements` method to ensure functions and classes are extracted correctly.
- Test the `_calculate_complexity` method to ensure cyclomatic complexity is calculated correctly.

### 5. Cache Module

**Improvements**:
- Add type hints for better readability and maintainability.
- Ensure all exceptions are logged with meaningful messages.
- Add more detailed docstrings for methods.

```python
class Cache:
    """Flexible caching system with optional Redis support."""

    def __init__(
        self,
        host: str = "localhost",
        port: int = 6379,
        db: int = 0,
        password: Optional[str] = None,
        enabled: bool = True,
        ttl: int = 3600,
        prefix: str = "docstring:"
    ):
        """
        Initialize cache with configuration.

        Args:
            host: Redis host
            port: Redis port
            db: Redis database number
            password: Redis password
            enabled: Whether caching is enabled
            ttl: Default TTL in seconds
            prefix: Cache key prefix
        """
        self.enabled = enabled
        self.ttl = ttl
        self.prefix = prefix
        self._stats = {
            'hits': 0,
            'misses': 0,
            'errors': 0
        }
        self._lock = asyncio.Lock()

        # Use in-memory cache by default
        self._cache: Dict[str, Dict[str, Any]] = {}
        self._timestamps: Dict[str, float] = {}

        # Initialize Redis if needed
        self._redis = None
        if enabled:
            self._init_redis(host, port, db, password)

    def _init_redis(
        self,
        host: str,
        port: int,
        db: int,
        password: Optional[str]
    ) -> None:
        """Initialize Redis connection if available."""
        try:
            import redis.asyncio as redis
            self._redis = redis.Redis(
                host=host,
                port=port,
                db=db,
                password=password,
                decode_responses=True
            )
            logger.info("Redis cache initialized")
        except ImportError:
            logger.warning("Redis not available, using in-memory cache")
        except Exception as e:
            logger.error(f"Redis initialization failed: {e}")

    @classmethod
    async def create(
        cls,
        host: str = "localhost",
        port: int = 6379,
        db: int = 0,
        password: Optional[str] = None,
        **kwargs
    ) -> 'Cache':
        """
        Create and initialize cache instance.

        Args:
            host: Redis host
            port: Redis port
            db: Redis database number
            password: Redis password
            **kwargs: Additional configuration

        Returns:
            Cache: Initialized cache instance
        """
        cache = cls(host, port, db, password, **kwargs)
        if cache._redis:
            try:
                await cache._redis.ping()
                logger.info("Redis connection verified")
            except Exception as e:
                logger.error(f"Redis connection failed: {e}")
                cache._redis = None
        return cache

    async def get_cached_docstring(
        self,
        key: str,
        default: Any = None
    ) -> Optional[Dict[str, Any]]:
        """
        Retrieve cached docstring by key.

        Args:
            key: Cache key
            default: Default value if not found

        Returns:
            Optional[Dict[str, Any]]: Cached data or default
        """
        if not self.enabled:
            return default

        cache_key = f"{self.prefix}{key}"
        try:
            # Try Redis first
            if self._redis:
                async with self._lock:
                    data = await self._redis.get(cache_key)
                if data:
                    self._stats['hits'] += 1
                    return json.loads(data)

            # Fallback to in-memory cache
            if cache_key in self._cache:
                if self._is_valid(cache_key):
                    self._stats['hits'] += 1
                    return self._cache[cache_key]
                else:
                    await self.invalidate(key)

            self._stats['misses'] += 1
            return default

        except Exception as e:
            self._stats['errors'] += 1
            logger.error(f"Cache get error: {e}")
            return default

    async def save_docstring(
        self,
        key: str,
        data: Dict[str, Any],
        ttl: Optional[int] = None
    ) -> bool:
        """
        Save docstring data to cache.

        Args:
            key: Cache key
            data: Data to cache
            ttl: Optional TTL override

        Returns:
            bool: Success status
        """
        if not self.enabled:
            return False

        cache_key = f"{self.prefix}{key}"
        expiration = ttl or self.ttl

        try:
            serialized = json.dumps(data)

            # Try Redis first
            if self._redis:
                async with self._lock:
                    await self._redis.set(
                        cache_key,
                        serialized,
                        ex=expiration
                    )

            # Always update in-memory cache as backup
            self._cache[cache_key] = data
            self._timestamps[cache_key] = time.time() + expiration
            return True

        except Exception as e:
            self._stats['errors'] += 1
            logger.error(f"Cache save error: {e}")
            return False

    async def invalidate(self, key: str) -> bool:
        """
        Invalidate cached entry.

        Args:
            key: Cache key

        Returns:
            bool: Success status
        """
        if not self.enabled:
            return False

        cache_key = f"{self.prefix}{key}"
        try:
            # Remove from Redis
            if self._redis:
                async with self._lock:
                    await self._redis.delete(cache_key)

            # Remove from in-memory cache
            self._cache.pop(cache_key, None)
            self._timestamps.pop(cache_key, None)
            return True

        except Exception as e:
            self._stats['errors'] += 1
            logger.error(f"Cache invalidation error: {e}")
            return False

    def _is_valid(self, key: str) -> bool:
        """Check if cached entry is still valid."""
        return time.time() < self._timestamps.get(key, 0)

    async def get_stats(self) -> Dict[str, Any]:
        """
        Get cache statistics.

        Returns:
            Dict[str, Any]: Cache statistics
        """
        stats = {
            'enabled': self.enabled,
            'stats': self._stats.copy()
        }

        if self._redis:
            try:
                info = await self._redis.info()
                stats.update({
                    'redis_connected': True,
                    'redis_memory_used': info.get('used_memory_human', 'N/A'),
                    'redis_clients': info.get('connected_clients', 0)
                })
            except Exception as e:
                logger.error(f"Error getting Redis stats: {e}")
                stats['redis_connected'] = False

        return stats

    async def clear(self) -> bool:
        """
        Clear all cached entries.

        Returns:
            bool: Success status
        """
        try:
            # Clear Redis
            if self._redis:
                async with self._lock:
                    keys = await self._redis.keys(f"{self.prefix}*")
                    if keys:
                        await self._redis.delete(*keys)

            # Clear in-memory cache
            self._cache.clear()
            self._timestamps.clear()
            return True

        except Exception as e:
            logger.error(f"Error clearing cache: {e}")
            return False

    async def close(self) -> None:
        """Close cache connections."""
        if self._redis:
            try:
                await self._redis.close()
                logger.info("Redis connection closed")
            except Exception as e:
                logger.error(f"Error closing Redis connection: {e}")

    async def __aenter__(self) -> 'Cache':
        """Async context manager entry."""
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb) -> None:
        """Async context manager exit."""
        await self.close()
```

**Suggested Test Cases**:
- Test the `get_cached_docstring` method to ensure cached docstrings are retrieved correctly.
- Test the `save_docstring` method to ensure docstrings are saved correctly.
- Test the `invalidate` method to ensure cached entries are invalidated correctly.
- Test the `get_stats` method to ensure cache statistics are accurate.

### 6. System Monitor Module

**Improvements**:
- Add type hints for better readability and maintainability.
- Ensure all exceptions are logged with meaningful messages.
- Add more detailed docstrings for methods.

```python
class SystemMonitor:
    """Monitors system resources and performance metrics."""

    def __init__(self, check_interval: int = 60):
        """
        Initialize system monitor.

        Args:
            check_interval: Interval between system checks in seconds
        """
        self.check_interval = check_interval
        self.start_time = datetime.now()
        self._metrics: Dict[str, List[Dict[str, Any]]] = defaultdict(list)
        self._running = False
        self._task: Optional[asyncio.Task] = None
        logger.info("System monitor initialized")

    async def start(self) -> None:
        """Start monitoring system resources."""
        if self._running:
            return

        self._running = True
        self._task = asyncio.create_task(self._monitor_loop())
        logger.info("System monitoring started")

    async def stop(self) -> None:
        """Stop monitoring system resources."""
        self._running = False
        if self._task:
            self._task.cancel()
            try:
                await self._task
            except asyncio.CancelledError:
                pass
        logger.info("System monitoring stopped")

    async def _monitor_loop(self) -> None:
        """Main monitoring loop."""
        while self._running:
            try:
                metrics = self._collect_system_metrics()
                self._store_metrics(metrics)
                await asyncio.sleep(self.check_interval)
            except Exception as e:
                logger.error(f"Error in monitoring loop: {e}")
                await asyncio.sleep(self.check_interval)

    def _collect_system_metrics(self) -> Dict[str, Any]:
        """Collect current system metrics."""
        try:
            cpu_percent = psutil.cpu_percent(interval=1)
            memory = psutil.virtual_memory()
            disk = psutil.disk_usage('/')

            return {
                'timestamp': datetime.now().isoformat(),
                'cpu': {
                    'percent': cpu_percent,
                    'count': psutil.cpu_count()
                },
                'memory': {
                    'total': memory.total,
                    'available': memory.available,
                    'percent': memory.percent
                },
                'disk': {
                    'total': disk.total,
                    'used': disk.used,
                    'free': disk.free,
                    'percent': disk.percent
                }
            }
        except Exception as e:
            logger.error(f"Error collecting system metrics: {e}")
            return {}

    def _store_metrics(self, metrics: Dict[str, Any]) -> None:
        """Store collected metrics."""
        for key, value in metrics.items():
            if key != 'timestamp':
                self._metrics[key].append({
                    'timestamp': metrics['timestamp'],
                    'value': value
                })

        # Keep only last hour of metrics
        max_entries = 3600 // self.check_interval
        for key in self._metrics:
            if len(self._metrics[key]) > max_entries:
                self._metrics[key] = self._metrics[key][-max_entries:]

    def get_metrics(self) -> Dict[str, Any]:
        """Get current metrics summary."""
        try:
            current_metrics = self._collect_system_metrics()
            runtime = (datetime.now() - self.start_time).total_seconds()

            return {
                'current': current_metrics,
                'runtime_seconds': runtime,
                'averages': self._calculate_averages(),
                'status': self._get_system_status()
            }
        except Exception as e:
            logger.error(f"Error getting metrics summary: {e}")
            return {'error': str(e)}

    def _calculate_averages(self) -> Dict[str, float]:
        """Calculate average values for metrics."""
        averages = {}
        for key, values in self._metrics.items():
            if values:
                if key == 'cpu':
                    averages[key] = sum(v['value']['percent'] for v in values) / len(values)
                elif key in ['memory', 'disk']:
                    averages[key] = sum(v['value']['percent'] for v in values) / len(values)
        return averages

    def _get_system_status(self) -> str:
        """Determine overall system status."""
        try:
            current = self._collect_system_metrics()

            # Define thresholds
            CPU_THRESHOLD = 90
            MEMORY_THRESHOLD = 90
            DISK_THRESHOLD = 90

            if (current.get('cpu', {}).get('percent', 0) > CPU_THRESHOLD or
                current.get('memory', {}).get('percent', 0) > MEMORY_THRESHOLD or
                current.get('disk', {}).get('percent', 0) > DISK_THRESHOLD):
                return 'critical'
            elif (current.get('cpu', {}).get('percent', 0) > CPU_THRESHOLD * 0.8 or
                  current.get('memory', {}).get('percent', 0) > MEMORY_THRESHOLD * 0.8 or
                  current.get('disk', {}).get('percent', 0) > DISK_THRESHOLD * 0.8):
                return 'warning'
            return 'healthy'
        except Exception as e:
            logger.error(f"Error getting system status: {e}")
            return 'unknown'

    async def __aenter__(self) -> 'SystemMonitor':
        """Async context manager entry."""
        await self.start()
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb) -> None:
        """Async context manager exit."""
        await self.stop()
```

**Suggested Test Cases**:
- Test the `start` and `stop` methods to ensure system monitoring starts and stops correctly.
- Test the `_collect_system_metrics` method to ensure system metrics are collected correctly.
- Test the `get_metrics` method to ensure the metrics summary is accurate.

### 7. Markdown Documentation Generator Module

**Improvements**:
- Add type hints for better readability and maintainability.
- Ensure all exceptions are logged with meaningful messages.
- Add more detailed docstrings for methods.

```python
class MarkdownDocumentationGenerator:
    """Generates standardized markdown documentation for Python code."""

    def __init__(self, source_code: str, module_path: Optional[str] = None):
        """
        Initialize markdown generator.

        Args:
            source_code: Source code to document
            module_path: Optional module path for documentation
        """
        self.source_code = source_code
        self.module_path = Path(module_path) if module_path else Path("module.py")
        self.tree = ast.parse(source_code)
        self.docstring = ast.get_docstring(self.tree) or ""
        self.changes: List[str] = []
        logger.info(f"Markdown generator initialized for {self.module_path}")

    async def generate_markdown(self) -> str:
        """
        Generate complete markdown documentation.

        Returns:
            str: Generated markdown documentation
        """
        try:
            sections = [
                await self._generate_header(),
                await self._generate_overview(),
                await self._generate_installation(),
                await self._generate_classes_section(),
                await self._generate_functions_section(),
                await self._generate_examples(),
                await self._generate_changes_section()
            ]

            return "\n\n".join(filter(None, sections))

        except Exception as e:
            logger.error(f"Failed to generate markdown: {e}")
            return f"# Documentation Generation Failed\n\nError: {str(e)}"

    async def _generate_header(self) -> str:
        """Generate module header section."""
        return f"# {self.module_path.stem}\n\n" + self._format_badges()

    def _format_badges(self) -> str:
        """Generate status badges."""
        return (
            "![Python](https://img.shields.io/badge/python-3.7%2B-blue)\n"
            "![Status](https://img.shields.io/badge/status-stable-green)\n"
            f"![Updated](https://img.shields.io/badge/updated-{datetime.now().strftime('%Y---%m')}-blue)\n"
        )

    async def _generate_overview(self) -> str:
        """Generate overview section."""
        overview = "## Overview\n\n"
        if self.docstring:
            overview += self.docstring
        else:
            overview += "*No module description available.*"
        return overview

    async def _generate_installation(self) -> str:
        """Generate installation instructions."""
        return (
            "## Installation\n\n"
            "```bash\n"
            f"pip install {self.module_path.stem}\n"
            "```"
        )

    async def _generate_classes_section(self) -> str:
        """Generate classes section with methods."""
        classes = [node for node in ast.walk(self.tree) if isinstance(node, ast.ClassDef)]
        if not classes:
            return ""

        sections = ["## Classes"]

        for cls in classes:
            sections.append(self._format_class(cls))

        return "\n\n".join(sections)

    def _format_class(self, cls: ast.ClassDef) -> str:
        """Format class documentation."""
        doc = f"### {cls.name}\n\n"

        # Add inheritance info
        if cls.bases:
            bases = ", ".join(ast.unparse(base) for base in cls.bases)
            doc += f"*Inherits from: `{bases}`*\n\n"

        # Add docstring
        if cls_doc := ast.get_docstring(cls):
            doc += f"{cls_doc}\n\n"

        # Add methods
        methods = [node for node in cls.body if isinstance(node, ast.FunctionDef)]
        if methods:
            doc += "#### Methods\n\n"
            for method in methods:
                doc += self._format_method(method)

        return doc

    def _format_method(self, method: ast.FunctionDef) -> str:
        """Format method documentation."""
        # Format signature
        signature = self._format_signature(method)
        doc = f"##### `{signature}`\n\n"

        # Add docstring
        if method_doc := ast.get_docstring(method):
            doc += f"{method_doc}\n\n"

        return doc

    async def _generate_functions_section(self) -> str:
        """Generate functions section."""
        functions = [
            node for node in ast.walk(self.tree)
            if isinstance(node, ast.FunctionDef)
            and not isinstance(node.parent, ast.ClassDef)
        ]

        if not functions:
            return ""

        sections = ["## Functions"]

        for func in functions:
            sections.append(self._format_function(func))

        return "\n\n".join(sections)

    def _format_function(self, func: ast.FunctionDef) -> str:
        """Format function documentation."""
        # Format signature
        signature = self._format_signature(func)
        doc = f"### `{signature}`\n\n"

        # Add docstring
        if func_doc := ast.get_docstring(func):
            doc += f"{func_doc}\n\n"

        return doc

    def _format_signature(self, node: ast.FunctionDef) -> str:
        """Format function/method signature."""
        args = []

        # Process arguments
        for arg in node.args.args:
            if arg.arg == 'self':
                continue

            arg_str = arg.arg
            if arg.annotation:
                arg_str += f": {ast.unparse(arg.annotation)}"
            args.append(arg_str)

        # Add return type
        returns = f" -> {ast.unparse(node.returns)}" if node.returns else ""

        return f"{node.name}({', '.join(args)}){returns}"

    async def _generate_examples(self) -> str:
        """Generate examples section."""
        return (
            "## Examples\n\n"
            "```python\n"
            f"from {self.module_path.stem} import *\n\n"
            "# Basic usage example\n"
            "...\n"
            "```"
        )

    async def _generate_changes_section(self) -> str:
        """Generate recent changes section."""
        if not self.changes:
            today = datetime.now().strftime('%Y-%m-%d')
            self.changes.append(f"[{today}] Initial documentation generated")

        return (
            "## Recent Changes\n\n" +
            "\n".join(f"- {change}" for change in self.changes)
        )

    def add_change(self, description: str) -> None:
        """
        Add a change entry to the documentation.

        Args:
            description: Change description
        """
        date = datetime.now().strftime('%Y-%m-%d')
        self.changes.append(f"[{date}] {description}")
```

**Suggested Test Cases**:
- Test the `generate_markdown` method to ensure markdown documentation is generated correctly.
- Test the `_generate_header` method to ensure the header section is generated correctly.
- Test the `_generate_overview` method to ensure the overview section is generated correctly.
- Test the `_generate_classes_section` method to ensure the classes section is generated correctly.
- Test the `_generate_functions_section` method to ensure the functions section is generated correctly.

### 8. Configuration Management Module

**Improvements**:
- Add type hints for better readability and maintainability.
- Ensure all exceptions are logged with meaningful messages.
- Add more detailed docstrings for methods.

```python
@dataclass
class AzureOpenAIConfig:
    """Configuration settings for Azure OpenAI integration."""

    # Azure OpenAI settings
    endpoint: str = field(default="")
    api_key: str = field(default="")
    api_version: str = field(default="2024-02-15-preview")
    deployment_name: str = field(default="")
    model_name: str = field(default="gpt-4")

    # Request settings
    max_tokens: int = field(default=1000)
    temperature: float = field(default=0.7)
    request_timeout: int = field(default=30)
    max_retries: int = field(default=3)
    retry_delay: int = field(default=2)

    # Rate limiting
    max_requests_per_minute: int = field(default=60)
    max_tokens_per_minute: int = field(default=150000)
    batch_size: int = field(default=5)

    # Caching settings
    cache_enabled: bool = field(default=True)
    cache_ttl: int = field(default=3600)
    redis_host: str = field(default="localhost")
    redis_port: int = field(default=6379)
    redis_db: int = field(default=0)
    redis_password: Optional[str] = field(default=None)

    # Logging settings
    log_level: str = field(default="INFO")
    log_format: str = field(
        default="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
    )
    log_directory: str = field(default="logs")

    @classmethod
    def from_env(cls) -> 'AzureOpenAIConfig':
        """
        Create configuration from environment variables.

        Returns:
            AzureOpenAIConfig: Configuration instance
        """
        try:
            config = cls(
                # Azure OpenAI settings
                endpoint=os.getenv("AZURE_OPENAI_ENDPOINT", ""),
                api_key=os.getenv("AZURE_OPENAI_KEY", ""),
                deployment_name=os.getenv("AZURE_OPENAI_DEPLOYMENT", ""),
                api_version=os.getenv("AZURE_OPENAI_API_VERSION", "2024-02-15-preview"),
                model_name=os.getenv("MODEL_NAME", "gpt-4"),

                # Request settings
                max_tokens=int(os.getenv("MAX_TOKENS", "1000")),
                temperature=float(os.getenv("TEMPERATURE", "0.7")),
                request_timeout=int(os.getenv("REQUEST_TIMEOUT", "30")),
                max_retries=int(os.getenv("MAX_RETRIES", "3")),
                retry_delay=int(os.getenv("RETRY_DELAY", "2")),

                # Rate limiting
                max_requests_per_minute=int(os.getenv("MAX_REQUESTS_PER_MINUTE", "60")),
                max_tokens_per_minute=int(os.getenv("MAX_TOKENS_PER_MINUTE", "150000")),
                batch_size=int(os.getenv("BATCH_SIZE", "5")),

                # Cache settings
                cache_enabled=os.getenv("CACHE_ENABLED", "true").lower() == "true",
                cache_ttl=int(os.getenv("CACHE_TTL", "3600")),
                redis_host=os.getenv("REDIS_HOST", "localhost"),
                redis_port=int(os.getenv("REDIS_PORT", "6379")),
                redis_db=int(os.getenv("REDIS_DB", "0")),
                redis_password=os.getenv("REDIS_PASSWORD"),

                # Logging settings
                log_level=os.getenv("LOG_LEVEL", "INFO"),
                log_format=os.getenv(
                    "LOG_FORMAT",
                    "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
                ),
                log_directory=os.getenv("LOG_DIRECTORY", "logs")
            )

            if not config.validate():
                raise ValueError("Invalid configuration")

            logger.info("Configuration loaded successfully")
            return config

        except Exception as e:
            logger.error(f"Error loading configuration: {e}")
            raise

    def validate(self) -> bool:
        """
        Validate configuration settings.

        Returns:
            bool: True if valid, False otherwise
        """
        try:
            # Validate required fields
            if not all([self.endpoint, self.api_key, self.deployment_name]):
                raise ValueError("Missing required Azure OpenAI credentials")

            # Validate numeric values
            if not 0 <= self.temperature <= 1:
                raise ValueError("Temperature must be between 0 and 1")

            if self.max_tokens <= 0:
                raise ValueError("Max tokens must be positive")

            # Validate rate limits
            if self.max_requests_per_minute <= 0:
                raise ValueError("Max requests per minute must be positive")

            # Validate cache settings
            if self.cache_enabled and self.cache_ttl <= 0:
                raise ValueError("Cache TTL must be positive")

            # Ensure log directory exists
            log_dir = Path(self.log_directory)
            log_dir.mkdir(parents=True, exist_ok=True)

            logger.info("Configuration validated successfully")
            return True

        except Exception as e:
            logger.error(f"Configuration validation failed: {e}")
            return False

    def to_dict(self) -> Dict[str, Any]:
        """
        Convert configuration to dictionary.

        Returns:
            Dict[str, Any]: Configuration as dictionary
        """
        return {
            # Azure OpenAI settings
            "endpoint": self.endpoint,
            "deployment_name": self.deployment_name,
            "model_name": self.model_name,
            "api_version": self.api_version,

            # Request settings
            "max_tokens": self.max_tokens,
            "temperature": self.temperature,
            "request_timeout": self.request_timeout,
            "max_retries": self.max_retries,

            # Rate limiting
            "max_requests_per_minute": self.max_requests_per_minute,
            "max_tokens_per_minute": self.max_tokens_per_minute,
            "batch_size": self.batch_size,

            # Cache settings
            "cache_enabled": self.cache_enabled,
            "cache_ttl": self.cache_ttl,
            "redis_host": self.redis_host,
            "redis_port": self.redis_port,
            "redis_db": self.redis_db,

            # Logging settings
            "log_level": self.log_level,
            "log_directory": self.log_directory
        }

    def update(self, **kwargs) -> None:
        """
        Update configuration settings.

        Args:
            **kwargs: Settings to update
        """
        for key, value in kwargs.items():
            if hasattr(self, key):
                setattr(self, key, value)
            else:
                logger.warning(f"Unknown configuration setting: {key}")

        if not self.validate():
            raise ValueError("Invalid configuration after update")
```

**Suggested Test Cases**:
- Test the `from_env` method to ensure configuration is loaded correctly from environment variables.
- Test the `validate` method to ensure configuration settings are validated correctly.
- Test the `to_dict` method to ensure the configuration is converted to a dictionary correctly.
- Test the `update` method to ensure configuration settings are updated correctly.

### 9. Response Parser Module

**Improvements**:
- Add type hints for better readability and maintainability.
- Ensure all exceptions are logged with meaningful messages.
- Add more detailed docstrings for methods.

```python
class ResponseParser:
    """Parses and validates Azure OpenAI API responses."""

    # Define response schema
    RESPONSE_SCHEMA = {
        "type": "object",
        "properties": {
            "documentation": {
                "type": "object",
                "properties": {
                    "summary": {"type": "string"},
                    "description": {"type": "string"},
                    "parameters": {
                        "type": "array",
                        "items": {
                            "type": "object",
                            "properties": {
                                "name": {"type": "string"},
                                "type": {"type": "string"},
                                "description": {"type": "string"}
                            },
                            "required": ["name", "type", "description"]
                        }
                    },
                    "returns": {
                        "type": "object",
                        "properties": {
                            "type": {"type": "string"},
                            "description": {"type": "string"}
                        },
                        "required": ["type", "description"]
                    }
                },
                "required": ["summary", "description"]
            }
        },
        "required": ["documentation"]
    }

    def __init__(self):
        """Initialize response parser."""
        self._validation_cache = {}
        logger.info("Response parser initialized")

    async def parse_response(
        self,
        response: str,
        expected_format: str = 'json'
    ) -> Optional[Dict[str, Any]]:
        """
        Parse and validate API response.

        Args:
            response: Raw API response
            expected_format: Expected response format ('json' or 'markdown')

        Returns:
            Optional[Dict[str, Any]]: Parsed response
        """
        try:
            if expected_format == 'json':
                return await self._parse_json_response(response)
            return await self._parse_markdown_response(response)
        except Exception as e:
            logger.error(f"Error parsing response: {e}")
            return None

    async def _parse_json_response(self, response: str) -> Optional[Dict[str, Any]]:
        """Parse JSON response."""
        try:
            # Handle potential markdown code blocks
            response = response.strip()
            if response.startswith('```json'):
                response = response[7:]
            if response.endswith('```'):
                response = response[:-3]

            # Parse JSON
            data = json.loads(response.strip())

            # Validate against schema
            if not self._validate_response(data):
                logger.error("Response validation failed")
                return None

            return data

        except json.JSONDecodeError as e:
            logger.error(f"JSON parsing error: {e}")
            return None
        except Exception as e:
            logger.error(f"Unexpected error parsing JSON response: {e}")
            return None

    async def _parse_markdown_response(self, response: str) -> Optional[Dict[str, Any]]:
        """Parse markdown response into structured format."""
        try:
            sections = self._split_markdown_sections(response)

            return {
                'documentation': {
                    'summary': sections.get('summary', ''),
                    'description': sections.get('description', ''),
                    'parameters': self._parse_parameters(sections.get('parameters', '')),
                    'returns': self._parse_returns(sections.get('returns', ''))
                }
            }

        except Exception as e:
            logger.error(f"Error parsing markdown response: {e}")
            return None

    def _validate_response(self, data: Dict[str, Any]) -> bool:
        """Validate response against schema."""
        try:
            validate(instance=data, schema=self.RESPONSE_SCHEMA)
            return True
        except ValidationError as e:
            logger.error(f"Validation error: {e}")
            return False

    def _split_markdown_sections(self, markdown: str) -> Dict[str, str]:
        """Split markdown into sections."""
        sections = {}
        current_section = 'description'
        current_content = []

        for line in markdown.split('\n'):
            if line.startswith('#'):
                # Save previous section
                if current_content:
                    sections[current_section] = '\n'.join(current_content).strip()
                    current_content = []

                # Update current section
                section_name = line.lstrip('#').strip().lower()
                current_section = section_name
            else:
                current_content.append(line)

        # Save final section
        if current_content:
            sections[current_section] = '\n'.join(current_content).strip()

        return sections

    def _parse_parameters(self, params_text: str) -> List[Dict[str, str]]:
        """Parse parameter section from markdown."""
        params = []
        current_param = None

        for line in params_text.split('\n'):
            line = line.strip()
            if line.startswith('- ') or line.startswith('* '):
                # New parameter
                if ':' in line:
                    name, rest = line[2:].split(':', 1)
                    current_param = {
                        'name': name.strip(),
                        'type': 'Any',
                        'description': rest.strip()
                    }
                    params.append(current_param)
            elif current_param and line:
                # Continue previous parameter description
                current_param['description'] += ' ' + line

        return params

    def _parse_returns(self, returns_text: str) -> Dict[str, str]:
        """Parse returns section from markdown."""
        if ':' in returns_text:
            type_str, description = returns_text.split(':', 1)
            return {
                'type': type_str.strip(),
                'description': description.strip()
            }
        return {
            'type': 'None',
            'description': returns_text.strip() or 'No return value.'
        }
```

**Suggested Test Cases**:
- Test the `parse_response` method to ensure API responses are parsed and validated correctly.
- Test the `_parse_json_response` method to ensure JSON responses are parsed and validated correctly.
- Test the `_parse_markdown_response` method to ensure markdown responses are parsed correctly.
- Test the `_split_markdown_sections` method to ensure markdown sections are split correctly.
- Test the `_parse_parameters` method to ensure parameters are parsed correctly.
- Test the `_parse_returns` method to ensure return values are parsed correctly.

## Conclusion

The provided codebase is comprehensive and well-structured, but there are always opportunities for improvement. The suggested changes focus on enhancing readability, maintainability, and performance while ensuring robust error handling and logging. The added type hints and detailed docstrings will make the code easier to understand and maintain. The suggested test cases will help ensure the code is reliable and behaves as expected.  
[Timestamp: 2024/11/24 10:05:51]

**ai**: I'll help you implement proper cleanup procedures with a comprehensive CleanupManager class and related utilities:

```python
"""
Cleanup Module

Provides comprehensive cleanup procedures for managing system resources,
temporary files, and stale data in the documentation generation system.
"""

import asyncio
import shutil
from datetime import datetime, timedelta
from pathlib import Path
from typing import List, Dict, Any, Set, Optional
from dataclasses import dataclass
import logging

from core.logger import LoggerSetup
from core.cache import Cache
from core.config import AzureOpenAIConfig
from exceptions import CleanupError

logger = LoggerSetup.get_logger(__name__)

@dataclass
class CleanupStats:
    """Statistics for cleanup operations."""
    cache_entries_removed: int = 0
    temp_files_removed: int = 0
    failed_operations_cleaned: int = 0
    disk_space_recovered: int = 0
    duration_seconds: float = 0.0

class CleanupManager:
    """Manages cleanup operations for the documentation generation system."""

    def __init__(
        self,
        cache: Optional[Cache],
        config: AzureOpenAIConfig,
        base_directory: Path
    ):
        """
        Initialize cleanup manager.

        Args:
            cache: Cache instance to clean
            config: Configuration settings
            base_directory: Base directory for cleanup operations
        """
        self.cache = cache
        self.config = config
        self.base_directory = Path(base_directory)
        self.temp_directory = self.base_directory / "temp"
        self.output_directory = self.base_directory / "output"
        self.stats = CleanupStats()

    async def perform_cleanup(
        self,
        max_cache_age: int = 86400,  # 24 hours
        max_temp_age: int = 3600,    # 1 hour
        dry_run: bool = False
    ) -> CleanupStats:
        """
        Perform comprehensive cleanup of system resources.

        Args:
            max_cache_age: Maximum age of cache entries in seconds
            max_temp_age: Maximum age of temporary files in seconds
            dry_run: If True, only simulate cleanup

        Returns:
            CleanupStats: Cleanup operation statistics

        Raises:
            CleanupError: If cleanup operation fails
        """
        start_time = datetime.now()
        
        try:
            # Cleanup tasks to run in parallel
            tasks = [
                self._cleanup_cache(max_cache_age, dry_run),
                self._cleanup_temp_files(max_temp_age, dry_run),
                self._cleanup_failed_operations(dry_run),
                self._cleanup_stale_output(dry_run)
            ]
            
            # Run cleanup tasks concurrently
            await asyncio.gather(*tasks)
            
            # Update cleanup duration
            self.stats.duration_seconds = (datetime.now() - start_time).total_seconds()
            
            # Log cleanup results
            self._log_cleanup_results()
            
            return self.stats

        except Exception as e:
            logger.error(f"Cleanup failed: {e}")
            raise CleanupError(f"Cleanup operation failed: {str(e)}")

    async def _cleanup_cache(self, max_age: int, dry_run: bool) -> None:
        """Clean up old cache entries."""
        if not self.cache:
            return

        try:
            # Get cache statistics before cleanup
            stats_before = await self.cache.get_stats()
            
            if not dry_run:
                # Get all cache keys
                cache_keys = await self._get_cache_keys()
                
                # Remove expired entries
                cutoff_time = datetime.now() - timedelta(seconds=max_age)
                for key in cache_keys:
                    if await self._is_cache_entry_expired(key, cutoff_time):
                        await self.cache.invalidate(key)
                        self.stats.cache_entries_removed += 1

            # Calculate space recovered
            stats_after = await self.cache.get_stats()
            self.stats.disk_space_recovered += self._calculate_space_difference(
                stats_before, stats_after
            )

        except Exception as e:
            logger.error(f"Cache cleanup failed: {e}")
            raise

    async def _cleanup_temp_files(self, max_age: int, dry_run: bool) -> None:
        """Clean up temporary files."""
        try:
            cutoff_time = datetime.now() - timedelta(seconds=max_age)
            
            # Ensure temp directory exists
            self.temp_directory.mkdir(parents=True, exist_ok=True)
            
            for file_path in self.temp_directory.rglob("*"):
                if file_path.is_file():
                    file_time = datetime.fromtimestamp(file_path.stat().st_mtime)
                    if file_time < cutoff_time:
                        if not dry_run:
                            file_size = file_path.stat().st_size
                            file_path.unlink()
                            self.stats.temp_files_removed += 1
                            self.stats.disk_space_recovered += file_size

        except Exception as e:
            logger.error(f"Temporary file cleanup failed: {e}")
            raise

    async def _cleanup_failed_operations(self, dry_run: bool) -> None:
        """Clean up artifacts from failed operations."""
        try:
            failed_ops_dir = self.base_directory / "failed_operations"
            if not failed_ops_dir.exists():
                return

            for op_dir in failed_ops_dir.iterdir():
                if op_dir.is_dir():
                    if not dry_run:
                        # Calculate directory size before removal
                        dir_size = self._get_directory_size(op_dir)
                        # Remove the directory
                        shutil.rmtree(op_dir)
                        self.stats.failed_operations_cleaned += 1
                        self.stats.disk_space_recovered += dir_size

        except Exception as e:
            logger.error(f"Failed operations cleanup failed: {e}")
            raise

    async def _cleanup_stale_output(self, dry_run: bool) -> None:
        """Clean up stale output files."""
        try:
            if not self.output_directory.exists():
                return

            # Get list of active documentation files
            active_files = await self._get_active_doc_files()

            for file_path in self.output_directory.rglob("*"):
                if file_path.is_file() and file_path.suffix in {'.md', '.py'}:
                    if file_path.stem not in active_files:
                        if not dry_run:
                            file_size = file_path.stat().st_size
                            file_path.unlink()
                            self.stats.disk_space_recovered += file_size

        except Exception as e:
            logger.error(f"Stale output cleanup failed: {e}")
            raise

    async def _get_cache_keys(self) -> Set[str]:
        """Get all cache keys."""
        if self.cache and hasattr(self.cache, '_redis'):
            keys = await self.cache._redis.keys(f"{self.cache.prefix}*")
            return {key.decode('utf-8') for key in keys}
        return set()

    async def _is_cache_entry_expired(self, key: str, cutoff_time: datetime) -> bool:
        """Check if cache entry is expired."""
        if self.cache:
            # Try to get timestamp from Redis
            if hasattr(self.cache, '_redis'):
                ttl = await self.cache._redis.ttl(key)
                if ttl > 0:
                    return False
            # Fallback to in-memory timestamp
            return not self.cache._is_valid(key)
        return True

    def _calculate_space_difference(
        self,
        before: Dict[str, Any],
        after: Dict[str, Any]
    ) -> int:
        """Calculate space difference between two stats snapshots."""
        try:
            before_size = before.get('redis_memory_used', 0)
            after_size = after.get('redis_memory_used', 0)
            return max(0, before_size - after_size)
        except Exception:
            return 0

    async def _get_active_doc_files(self) -> Set[str]:
        """Get set of active documentation file names."""
        active_files = set()
        source_dir = self.base_directory / "source"
        if source_dir.exists():
            for file_path in source_dir.rglob("*.py"):
                active_files.add(file_path.stem)
        return active_files

    def _get_directory_size(self, directory: Path) -> int:
        """Calculate total size of a directory."""
        return sum(f.stat().st_size for f in directory.rglob('*') if f.is_file())

    def _log_cleanup_results(self) -> None:
        """Log cleanup operation results."""
        logger.info("Cleanup completed successfully:")
        logger.info(f"Cache entries removed: {self.stats.cache_entries_removed}")
        logger.info(f"Temporary files removed: {self.stats.temp_files_removed}")
        logger.info(f"Failed operations cleaned: {self.stats.failed_operations_cleaned}")
        logger.info(f"Disk space recovered: {self.stats.disk_space_recovered} bytes")
        logger.info(f"Duration: {self.stats.duration_seconds:.2f} seconds")

class AutoCleanupManager:
    """Manages automatic cleanup scheduling."""

    def __init__(
        self,
        cleanup_manager: CleanupManager,
        interval_hours: int = 24
    ):
        """
        Initialize auto cleanup manager.

        Args:
            cleanup_manager: CleanupManager instance
            interval_hours: Cleanup interval in hours
        """
        self.cleanup_manager = cleanup_manager
        self.interval = interval_hours * 3600  # Convert to seconds
        self._task: Optional[asyncio.Task] = None
        self._running = False

    async def start(self) -> None:
        """Start automatic cleanup schedule."""
        if self._running:
            return

        self._running = True
        self._task = asyncio.create_task(self._cleanup_loop())
        logger.info(f"Automatic cleanup scheduled every {self.interval/3600:.1f} hours")

    async def stop(self) -> None:
        """Stop automatic cleanup schedule."""
        self._running = False
        if self._task:
            self._task.cancel()
            try:
                await self._task
            except asyncio.CancelledError:
                pass
        logger.info("Automatic cleanup stopped")

    async def _cleanup_loop(self) -> None:
        """Main cleanup loop."""
        while self._running:
            try:
                await self.cleanup_manager.perform_cleanup()
                await asyncio.sleep(self.interval)
            except Exception as e:
                logger.error(f"Automatic cleanup failed: {e}")
                await asyncio.sleep(self.interval)

    async def __aenter__(self) -> 'AutoCleanupManager':
        """Async context manager entry."""
        await self.start()
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb) -> None:
        """Async context manager exit."""
        await self.stop()
```

This implementation provides:

1. **Comprehensive Cleanup**:
   - Cache entries cleanup
   - Temporary files cleanup
   - Failed operations cleanup
   - Stale output cleanup

2. **Statistics Tracking**:
   - Number of items cleaned
   - Disk space recovered
   - Operation duration

3. **Safety Features**:
   - Dry run option
   - Error handling
   - Logging
   - Concurrent cleanup operations

4. **Automatic Cleanup**:
   - Scheduled cleanup
   - Configurable intervals
   - Async context manager support

Usage example:

```python
async def main():
    # Initialize components
    config = AzureOpenAIConfig.from_env()
    cache = await Cache.create()
    base_dir = Path("./documentation")
    
    # Create cleanup manager
    cleanup_manager = CleanupManager(cache, config, base_dir)
    
    # Perform manual cleanup
    try:
        stats = await cleanup_manager.perform_cleanup(
            max_cache_age=86400,  # 24 hours
            max_temp_age=3600,    # 1 hour
            dry_run=False
        )
        print(f"Cleanup completed: {stats}")
    except CleanupError as e:
        print(f"Cleanup failed: {e}")
    
    # Or use automatic cleanup
    async with AutoCleanupManager(cleanup_manager, interval_hours=24) as auto_cleanup:
        # Your main application code here
        pass

if __name__ == "__main__":
    asyncio.run(main())
```

This implementation ensures proper resource management and system maintenance while providing detailed statistics and logging for monitoring purposes.  
[Timestamp: 2024/11/24 10:06:17]

**user**: Explain what you’ve done with each module  
[Timestamp: 2024/11/24 10:18:21]

**ai**: I'll explain the key components and functionality of each module in the cleanup implementation:

1. **CleanupStats Class**

```python
@dataclass
class CleanupStats:
    """Statistics for cleanup operations."""
    cache_entries_removed: int = 0
    temp_files_removed: int = 0
    failed_operations_cleaned: int = 0
    disk_space_recovered: int = 0
    duration_seconds: float = 0.0
```

- Tracks cleanup operation statistics
- Uses dataclass for clean, immutable data structure
- Captures key metrics like items removed and space recovered

2. **CleanupManager Class**

```python
class CleanupManager:
    def __init__(
        self,
        cache: Optional[Cache],
        config: AzureOpenAIConfig,
        base_directory: Path
    ):
        self.cache = cache
        self.config = config
        self.base_directory = Path(base_directory)
        self.temp_directory = self.base_directory / "temp"
        self.output_directory = self.base_directory / "output"
        self.stats = CleanupStats()
```

Core functionality:
- Manages all cleanup operations
- Handles different types of cleanup (cache, temp files, failed operations)
- Maintains cleanup statistics
- Provides both manual and automated cleanup options

3. **Main Cleanup Method**

```python
async def perform_cleanup(
    self,
    max_cache_age: int = 86400,  # 24 hours
    max_temp_age: int = 3600,    # 1 hour
    dry_run: bool = False
) -> CleanupStats:
    start_time = datetime.now()
    
    try:
        # Parallel cleanup tasks
        tasks = [
            self._cleanup_cache(max_cache_age, dry_run),
            self._cleanup_temp_files(max_temp_age, dry_run),
            self._cleanup_failed_operations(dry_run),
            self._cleanup_stale_output(dry_run)
        ]
        
        await asyncio.gather(*tasks)
        
        self.stats.duration_seconds = (datetime.now() - start_time).total_seconds()
        self._log_cleanup_results()
        
        return self.stats
```

Features:
- Concurrent cleanup operations
- Configurable retention periods
- Dry run option for safe testing
- Comprehensive error handling
- Detailed logging

4. **Cache Cleanup**

```python
async def _cleanup_cache(self, max_age: int, dry_run: bool) -> None:
    if not self.cache:
        return

    try:
        stats_before = await self.cache.get_stats()
        
        if not dry_run:
            cache_keys = await self._get_cache_keys()
            cutoff_time = datetime.now() - timedelta(seconds=max_age)
            
            for key in cache_keys:
                if await self._is_cache_entry_expired(key, cutoff_time):
                    await self.cache.invalidate(key)
                    self.stats.cache_entries_removed += 1

        stats_after = await self.cache.get_stats()
        self.stats.disk_space_recovered += self._calculate_space_difference(
            stats_before, stats_after
        )
```

Features:
- Removes expired cache entries
- Calculates space recovered
- Handles both Redis and in-memory cache
- Safe cleanup with proper error handling

5. **Temporary File Cleanup**

```python
async def _cleanup_temp_files(self, max_age: int, dry_run: bool) -> None:
    try:
        cutoff_time = datetime.now() - timedelta(seconds=max_age)
        
        self.temp_directory.mkdir(parents=True, exist_ok=True)
        
        for file_path in self.temp_directory.rglob("*"):
            if file_path.is_file():
                file_time = datetime.fromtimestamp(file_path.stat().st_mtime)
                if file_time < cutoff_time:
                    if not dry_run:
                        file_size = file_path.stat().st_size
                        file_path.unlink()
                        self.stats.temp_files_removed += 1
                        self.stats.disk_space_recovered += file_size
```

Features:
- Removes old temporary files
- Tracks file sizes and counts
- Recursive directory scanning
- Safe file deletion

6. **Failed Operations Cleanup**

```python
async def _cleanup_failed_operations(self, dry_run: bool) -> None:
    try:
        failed_ops_dir = self.base_directory / "failed_operations"
        if not failed_ops_dir.exists():
            return

        for op_dir in failed_ops_dir.iterdir():
            if op_dir.is_dir():
                if not dry_run:
                    dir_size = self._get_directory_size(op_dir)
                    shutil.rmtree(op_dir)
                    self.stats.failed_operations_cleaned += 1
                    self.stats.disk_space_recovered += dir_size
```

Features:
- Cleans up failed operation artifacts
- Calculates directory sizes
- Safe directory removal
- Tracks cleanup statistics

7. **AutoCleanupManager Class**

```python
class AutoCleanupManager:
    def __init__(
        self,
        cleanup_manager: CleanupManager,
        interval_hours: int = 24
    ):
        self.cleanup_manager = cleanup_manager
        self.interval = interval_hours * 3600
        self._task: Optional[asyncio.Task] = None
        self._running = False

    async def _cleanup_loop(self) -> None:
        while self._running:
            try:
                await self.cleanup_manager.perform_cleanup()
                await asyncio.sleep(self.interval)
            except Exception as e:
                logger.error(f"Automatic cleanup failed: {e}")
                await asyncio.sleep(self.interval)
```

Features:
- Automated periodic cleanup
- Configurable cleanup intervals
- Async context manager support
- Error handling and recovery

8. **Utility Methods**

```python
def _calculate_space_difference(
    self,
    before: Dict[str, Any],
    after: Dict[str, Any]
) -> int:
    try:
        before_size = before.get('redis_memory_used', 0)
        after_size = after.get('redis_memory_used', 0)
        return max(0, before_size - after_size)
    except Exception:
        return 0

def _log_cleanup_results(self) -> None:
    logger.info("Cleanup completed successfully:")
    logger.info(f"Cache entries removed: {self.stats.cache_entries_removed}")
    logger.info(f"Temporary files removed: {self.stats.temp_files_removed}")
    logger.info(f"Failed operations cleaned: {self.stats.failed_operations_cleaned}")
    logger.info(f"Disk space recovered: {self.stats.disk_space_recovered} bytes")
    logger.info(f"Duration: {self.stats.duration_seconds:.2f} seconds")
```

Features:
- Helper methods for common tasks
- Detailed logging
- Space calculation utilities
- Error handling

The implementation provides:

1. **Comprehensive Cleanup**:
   - Multiple cleanup types
   - Concurrent operations
   - Configurable retention periods

2. **Safety Features**:
   - Dry run mode
   - Error handling
   - Logging
   - Atomic operations

3. **Monitoring**:
   - Detailed statistics
   - Operation tracking
   - Space recovery calculation

4. **Automation**:
   - Scheduled cleanup
   - Configurable intervals
   - Async operation

5. **Extensibility**:
   - Modular design
   - Easy to add new cleanup types
   - Configurable behavior

This implementation ensures proper resource management while providing detailed insights into cleanup operations and system maintenance.  
[Timestamp: 2024/11/24 10:19:13]

**user**: Provide a file structure mapped to the classes  
[Timestamp: 2024/11/24 10:20:28]

**ai**: Here's a detailed file structure mapping for the codebase, organized by functionality:

```plaintext
src/
├── core/
│   ├── __init__.py
│   ├── logger.py              # LoggerSetup class
│   ├── config.py              # AzureOpenAIConfig class
│   ├── cache.py               # Cache class
│   ├── monitoring/
│   │   ├── __init__.py
│   │   ├── metrics.py         # MetricsCollector class
│   │   └── system.py          # SystemMonitor class
│   └── cleanup/
│       ├── __init__.py
│       ├── manager.py         # CleanupManager class
│       ├── auto_cleanup.py    # AutoCleanupManager class
│       └── stats.py           # CleanupStats class
│
├── analysis/
│   ├── __init__.py
│   ├── ast_analyzer.py        # ASTAnalyzer class
│   ├── extraction/
│   │   ├── __init__.py
│   │   ├── base.py           # BaseExtractor class
│   │   └── manager.py        # ExtractionManager class
│   └── complexity.py         # ComplexityAnalyzer class
│
├── documentation/
│   ├── __init__.py
│   ├── generator.py          # MarkdownDocumentationGenerator class
│   ├── validator.py          # DocstringValidator class
│   └── parser.py            # ResponseParser class
│
├── ai/
│   ├── __init__.py
│   ├── handler.py           # AIInteractionHandler class
│   └── prompts/
│       ├── __init__.py
│       └── templates.py     # Prompt templates
│
├── utils/
│   ├── __init__.py
│   ├── file_handler.py      # FileHandler class
│   ├── code_utils.py        # CodeAnalysisUtils class
│   └── hash_utils.py        # HashUtils class
│
└── main.py                  # Main workflow orchestration

tests/
├── __init__.py
├── conftest.py             # Test configurations and fixtures
├── core/
│   ├── test_config.py
│   ├── test_cache.py
│   └── test_cleanup.py
├── analysis/
│   ├── test_ast_analyzer.py
│   └── test_extraction.py
├── documentation/
│   ├── test_generator.py
│   └── test_validator.py
└── ai/
    └── test_handler.py

docs/
├── api/                    # API documentation
├── examples/               # Usage examples
└── workflow/              # Workflow documentation
```

Here's how the classes map to their respective files:

1. **Core Module**

```python
# core/logger.py
class LoggerSetup:
    """Handles logging configuration and setup."""

# core/config.py
@dataclass
class AzureOpenAIConfig:
    """Configuration settings for Azure OpenAI integration."""

# core/cache.py
class Cache:
    """Flexible caching system with optional Redis support."""

# core/monitoring/metrics.py
class MetricsCollector:
    """Collects and manages various performance metrics."""

# core/monitoring/system.py
class SystemMonitor:
    """Monitors system resources and performance metrics."""

# core/cleanup/manager.py
class CleanupManager:
    """Manages cleanup operations for the system."""

# core/cleanup/auto_cleanup.py
class AutoCleanupManager:
    """Handles automated cleanup operations."""

# core/cleanup/stats.py
@dataclass
class CleanupStats:
    """Statistics for cleanup operations."""
```

2. **Analysis Module**

```python
# analysis/ast_analyzer.py
class ASTAnalyzer:
    """Analyzes Python code using AST."""

# analysis/extraction/base.py
class BaseExtractor:
    """Base class for code extraction."""

# analysis/extraction/manager.py
class ExtractionManager:
    """Manages extraction of code elements."""

# analysis/complexity.py
class ComplexityAnalyzer:
    """Analyzes code complexity."""
```

3. **Documentation Module**

```python
# documentation/generator.py
class MarkdownDocumentationGenerator:
    """Generates markdown documentation."""

# documentation/validator.py
class DocstringValidator:
    """Validates and processes docstrings."""

# documentation/parser.py
class ResponseParser:
    """Parses and validates API responses."""
```

4. **AI Module**

```python
# ai/handler.py
class AIInteractionHandler:
    """Manages AI model interactions."""

# ai/prompts/templates.py
class PromptTemplate:
    """Template management for AI prompts."""
```

5. **Utils Module**

```python
# utils/file_handler.py
class FileHandler:
    """Handles file operations."""

# utils/code_utils.py
class CodeAnalysisUtils:
    """Utilities for code analysis."""

# utils/hash_utils.py
class HashUtils:
    """Utilities for hashing and caching."""
```

6. **Main Workflow**

```python
# main.py
class WorkflowOrchestrator:
    """Orchestrates the documentation generation workflow."""

class DocumentationProcessor:
    """Handles the core documentation processing logic."""

class AsyncComponentManager:
    """Manages async components lifecycle."""
```

7. **Tests**

```python
# tests/conftest.py
# Common test fixtures and configurations

# tests/core/test_config.py
class TestAzureOpenAIConfig:
    """Tests for configuration management."""

# tests/core/test_cache.py
class TestCache:
    """Tests for caching system."""

# tests/analysis/test_ast_analyzer.py
class TestASTAnalyzer:
    """Tests for AST analysis."""

# tests/documentation/test_generator.py
class TestMarkdownGenerator:
    """Tests for documentation generation."""
```

This structure provides:

1. **Modularity**:
   - Clear separation of concerns
   - Easy to maintain and extend
   - Logical grouping of related functionality

2. **Testing**:
   - Parallel structure for tests
   - Easy to locate test files
   - Organized test fixtures

3. **Documentation**:
   - API documentation
   - Usage examples
   - Workflow documentation

4. **Utilities**:
   - Common utilities grouped together
   - Shared functionality accessible

5. **Configuration**:
   - Central configuration management
   - Environment-based settings

To use this structure:

1. Import classes using absolute imports:

```python
from core.config import AzureOpenAIConfig
from core.cache import Cache
from documentation.generator import MarkdownDocumentationGenerator
```

2. Run tests by module:

```bash
pytest tests/core/
pytest tests/analysis/
pytest tests/documentation/
```

3. Generate documentation:

```bash
python -m sphinx.cmd.build -b html docs/source/ docs/build/
```

This structure makes the codebase:
- Easy to navigate
- Simple to maintain
- Well-organized
- Testable
- Documented
- Extensible  
[Timestamp: 2024/11/24 10:21:07]

**user**: This is too ambitious  
[Timestamp: 2024/11/24 10:21:23]

**ai**: You're right. Let's create a simpler, more focused structure that's easier to maintain and understand. Here's a streamlined version:

```plaintext
docstring_generator/
├── __init__.py
├── core/
│   ├── __init__.py
│   ├── config.py          # Configuration management
│   ├── logger.py          # Logging setup
│   └── metrics.py         # Basic metrics collection
│
├── generators/
│   ├── __init__.py
│   ├── ast_parser.py      # AST analysis
│   ├── docstring.py       # Docstring generation
│   └── markdown.py        # Markdown formatting
│
├── utils/
│   ├── __init__.py
│   └── helpers.py         # Common utilities
│
└── main.py               # Main entry point

tests/
├── __init__.py
├── test_ast_parser.py
├── test_docstring.py
└── test_markdown.py
```

Key Classes:

1. **Configuration and Logging**

```python
# core/config.py
class Config:
    """Simple configuration management."""
    def __init__(self):
        self.openai_key = os.getenv("OPENAI_API_KEY")
        self.model = os.getenv("OPENAI_MODEL", "gpt-4")
        self.max_tokens = int(os.getenv("MAX_TOKENS", "1000"))

# core/logger.py
class Logger:
    """Basic logging setup."""
    def __init__(self, name: str):
        self.logger = logging.getLogger(name)
```

2. **AST Parser**

```python
# generators/ast_parser.py
class ASTParser:
    """Parses Python code using AST."""
    def parse_code(self, source_code: str) -> Dict[str, Any]:
        """Extract function and class information."""
        tree = ast.parse(source_code)
        return {
            'functions': self._extract_functions(tree),
            'classes': self._extract_classes(tree)
        }
```

3. **Docstring Generator**

```python
# generators/docstring.py
class DocstringGenerator:
    """Generates docstrings using OpenAI."""
    def __init__(self, config: Config):
        self.config = config
        self.client = OpenAI(api_key=config.openai_key)

    async def generate_docstring(self, code: str) -> str:
        """Generate docstring for given code."""
        response = await self.client.chat.completions.create(
            model=self.config.model,
            messages=[
                {"role": "system", "content": "Generate docstring for this code:"},
                {"role": "user", "content": code}
            ]
        )
        return response.choices[0].message.content
```

4. **Markdown Generator**

```python
# generators/markdown.py
class MarkdownGenerator:
    """Generates markdown documentation."""
    def generate_docs(self, parsed_code: Dict[str, Any]) -> str:
        """Create markdown documentation."""
        sections = [
            self._generate_header(),
            self._generate_functions(parsed_code['functions']),
            self._generate_classes(parsed_code['classes'])
        ]
        return "\n\n".join(sections)
```

5. **Main Workflow**

```python
# main.py
class DocGenerator:
    """Main documentation generation workflow."""
    def __init__(self):
        self.config = Config()
        self.parser = ASTParser()
        self.docstring_gen = DocstringGenerator(self.config)
        self.markdown_gen = MarkdownGenerator()

    async def process_file(self, file_path: str) -> str:
        """Process a single file."""
        with open(file_path) as f:
            code = f.read()
        
        parsed_code = self.parser.parse_code(code)
        docstring = await self.docstring_gen.generate_docstring(code)
        return self.markdown_gen.generate_docs({
            **parsed_code,
            'docstring': docstring
        })

# Command-line interface
async def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("file_path", help="Python file to process")
    args = parser.parse_args()

    generator = DocGenerator()
    docs = await generator.process_file(args.file_path)
    print(docs)

if __name__ == "__main__":
    asyncio.run(main())
```

This simplified structure:

1. **Focuses on Core Functionality**:
   - AST parsing
   - Docstring generation
   - Markdown formatting

2. **Minimizes Dependencies**:
   - Basic configuration
   - Simple logging
   - Essential utilities

3. **Easy to Test**:
   - Clear component boundaries
   - Simple interfaces
   - Focused functionality

4. **Simple to Use**:

```bash
# Set environment variables
export OPENAI_API_KEY="your-key-here"

# Run the tool
python -m docstring_generator.main your_file.py
```

5. **Easy to Extend**:
   - Add new generators
   - Enhance parsing
   - Improve formatting

This version is:
- More manageable
- Easier to understand
- Simpler to maintain
- Focused on essential features
- Quick to implement

Would you like me to expand on any particular part of this simplified structure?  
[Timestamp: 2024/11/24 10:21:55]
